{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "sd = torch.load('/home/yujiepan/work2/jpqd-vit/LOGS/ww42/1012.689a-vit-jpqnd-wt0wr0.060-prune2to7f8-epo45lr5e-5wd1e-6_2card/model_43.pth')['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "new_sd = deepcopy(sd)\n",
    "for i in range(12):\n",
    "    for m in [\"q\", \"k\", \"v\", \"out\"]:\n",
    "        keys.append(f\"nncf_module.encoder.layers.encoder_layer_{i}.self_attention.{m}_proj\")\n",
    "    for m in [0, 3]:\n",
    "        keys.append(f\"nncf_module.encoder.layers.encoder_layer_{i}.mlp.{m}\")\n",
    "\n",
    "for key in keys:\n",
    "    for d in [\"weight\", \"bias\"]:\n",
    "        importance = f\"{key}.pre_ops.0.op._{d}_importance\"\n",
    "        # print('patching', importance)\n",
    "        new_sd[importance] = torch.ones_like(sd[importance]) * 100.0\n",
    "        new_sd[f\"{key}.{d}\"] = sd[f\"{key}.{d}\"] * sd[f\"{key}.pre_ops.0.op.{d}_ctx._binary_mask\"]\n",
    "        new_sd[f\"{key}.pre_ops.0.op.{d}_ctx._binary_mask\"] = torch.ones_like(sd[f\"{key}.pre_ops.0.op.{d}_ctx._binary_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "def show(tensor):\n",
    "    imgs = tensor\n",
    "    for i in range(3 - len(tensor.shape)):\n",
    "        imgs = imgs.unsqueeze(0)\n",
    "    imgs = torchvision.utils.make_grid(imgs)\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0, 1, 7, 9, 11] 826\n",
      "1 [2, 4, 8] 1508\n",
      "2 [5, 9, 10, 11] 2100\n",
      "3 [0, 2, 3, 5, 6, 7, 9, 10] 2143\n",
      "4 [0, 1, 3, 5, 6, 7, 8, 9, 10, 11] 2157\n",
      "5 [0, 3, 4, 5, 7, 8, 9, 10, 11] 2167\n",
      "6 [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11] 2219\n",
      "7 [0, 1, 2, 3, 5, 8, 9, 10, 11] 2374\n",
      "8 [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11] 2147\n",
      "9 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] 2329\n",
      "10 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] 2857\n",
      "11 [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11] 1594\n"
     ]
    }
   ],
   "source": [
    "keys = []\n",
    "\n",
    "def need_to_preserve_head(layer_id, head_id, new_sd):\n",
    "    for m in [\"q\", \"k\", \"v\"]:\n",
    "        weight = new_sd[f\"nncf_module.encoder.layers.encoder_layer_{layer_id}.self_attention.{m}_proj.weight\"]\n",
    "        if (weight[head_id*64 : (head_id+1)*64, :].abs() > 1e-10).float().sum() > 0:\n",
    "            # print(m)\n",
    "            return True\n",
    "        bias = new_sd[f\"nncf_module.encoder.layers.encoder_layer_{layer_id}.self_attention.{m}_proj.bias\"]\n",
    "        bias = bias.reshape(-1)\n",
    "        if (bias[head_id*64 : (head_id+1)*64].abs() > 1e-10).float().sum() > 0:\n",
    "            # print(m, 'bias')\n",
    "            return True\n",
    "    m = 'out'\n",
    "    weight = new_sd[f\"nncf_module.encoder.layers.encoder_layer_{layer_id}.self_attention.{m}_proj.weight\"]\n",
    "    if (weight[:, head_id*64 : (head_id+1)*64].abs() > 1e-10).float().sum() > 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def need_to_preserve_ffn(layer_id, ffn_id, new_sd):\n",
    "    weight = new_sd[f'nncf_module.encoder.layers.encoder_layer_{layer_id}.mlp.0.weight']\n",
    "    if (weight[ffn_id, :].abs() > 1e-10).float().sum() > 0:\n",
    "        return True\n",
    "    weight = new_sd[f'nncf_module.encoder.layers.encoder_layer_{layer_id}.mlp.3.weight']\n",
    "    if (weight[:, ffn_id].abs() > 1e-10).float().sum() > 0:\n",
    "        return True\n",
    "    bias = new_sd[f'nncf_module.encoder.layers.encoder_layer_{layer_id}.mlp.0.bias']\n",
    "    bias = bias.reshape(-1)\n",
    "    if (bias[ffn_id].abs() > 1e-10).float().sum() > 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "for layer_id in range(12):\n",
    "    preserved_heads = []\n",
    "    preserved_ffns = []\n",
    "    for head_id in range(12):\n",
    "        if need_to_preserve_head(layer_id, head_id, new_sd):\n",
    "            preserved_heads.append(head_id)\n",
    "    for ffn_id in range(768 * 4):\n",
    "        if need_to_preserve_ffn(layer_id, ffn_id, new_sd):\n",
    "            preserved_ffns.append(ffn_id)\n",
    "    print(layer_id, preserved_heads, len(preserved_ffns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 1\n",
    "import torchvision\n",
    "m = 0\n",
    "\n",
    "x = sd[f'nncf_module.encoder.layers.encoder_layer_{i}.mlp.{m}.weight']\n",
    "show((x.abs() > 2e-10).float())\n",
    "print(x.shape)\n",
    "\n",
    "show((sd[f'nncf_module.encoder.layers.encoder_layer_{i}.mlp.{m}.pre_ops.0.op.bias_ctx._binary_mask']).repeat(800, 1))\n",
    "y = sd[f'nncf_module.encoder.layers.encoder_layer_{i}.mlp.{m}.pre_ops.0.op.bias_ctx._binary_mask']\n",
    "x = sd[f'nncf_module.encoder.layers.encoder_layer_{i}.mlp.{m}.pre_ops.0.op.weight_ctx._binary_mask']\n",
    "x.shape, y.shape\n",
    "show(x)\n",
    "torch.testing.assert_allclose(x.mean(dim=1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    model     acc  sparsity      drop\n",
      "14  ww42/1008.713e_40.pth  80.654  0.255133 -0.510682\n",
      "11  ww42/1008.713e_41.pth  80.718  0.255133 -0.431736\n",
      "13  ww42/1008.713e_42.pth  80.640  0.255133 -0.527952\n",
      "12  ww42/1008.713e_43.pth  80.690  0.255133 -0.466275\n",
      "8   ww42/1008.713e_44.pth  80.664  0.255133 -0.498347\n",
      "6   ww42/1011.6586_40.pth  80.470  0.303244 -0.737652\n",
      "9   ww42/1011.6586_41.pth  80.332  0.303244 -0.907880\n",
      "1   ww42/1011.6586_42.pth  80.368  0.303244 -0.863473\n",
      "5   ww42/1011.6586_43.pth  80.370  0.303244 -0.861006\n",
      "4   ww42/1011.6586_44.pth  80.422  0.303244 -0.796862\n",
      "2   ww42/1012.689a_40.pth  80.214  0.317545 -1.053437\n",
      "3   ww42/1012.689a_41.pth  80.302  0.317545 -0.944886\n",
      "7   ww42/1012.689a_42.pth  80.248  0.317545 -1.011497\n",
      "10  ww42/1012.689a_43.pth  80.320  0.317545 -0.922682\n",
      "0   ww42/1012.689a_44.pth  80.216  0.317545 -1.050970\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>acc</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>drop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ww42/1008.713e_40.pth</td>\n",
       "      <td>80.654</td>\n",
       "      <td>0.255133</td>\n",
       "      <td>-0.510682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ww42/1008.713e_41.pth</td>\n",
       "      <td>80.718</td>\n",
       "      <td>0.255133</td>\n",
       "      <td>-0.431736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ww42/1008.713e_42.pth</td>\n",
       "      <td>80.640</td>\n",
       "      <td>0.255133</td>\n",
       "      <td>-0.527952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ww42/1008.713e_43.pth</td>\n",
       "      <td>80.690</td>\n",
       "      <td>0.255133</td>\n",
       "      <td>-0.466275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ww42/1008.713e_44.pth</td>\n",
       "      <td>80.664</td>\n",
       "      <td>0.255133</td>\n",
       "      <td>-0.498347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ww42/1011.6586_40.pth</td>\n",
       "      <td>80.470</td>\n",
       "      <td>0.303244</td>\n",
       "      <td>-0.737652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ww42/1011.6586_41.pth</td>\n",
       "      <td>80.332</td>\n",
       "      <td>0.303244</td>\n",
       "      <td>-0.907880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ww42/1011.6586_42.pth</td>\n",
       "      <td>80.368</td>\n",
       "      <td>0.303244</td>\n",
       "      <td>-0.863473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ww42/1011.6586_43.pth</td>\n",
       "      <td>80.370</td>\n",
       "      <td>0.303244</td>\n",
       "      <td>-0.861006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ww42/1011.6586_44.pth</td>\n",
       "      <td>80.422</td>\n",
       "      <td>0.303244</td>\n",
       "      <td>-0.796862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ww42/1012.689a_40.pth</td>\n",
       "      <td>80.214</td>\n",
       "      <td>0.317545</td>\n",
       "      <td>-1.053437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ww42/1012.689a_41.pth</td>\n",
       "      <td>80.302</td>\n",
       "      <td>0.317545</td>\n",
       "      <td>-0.944886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ww42/1012.689a_42.pth</td>\n",
       "      <td>80.248</td>\n",
       "      <td>0.317545</td>\n",
       "      <td>-1.011497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ww42/1012.689a_43.pth</td>\n",
       "      <td>80.320</td>\n",
       "      <td>0.317545</td>\n",
       "      <td>-0.922682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ww42/1012.689a_44.pth</td>\n",
       "      <td>80.216</td>\n",
       "      <td>0.317545</td>\n",
       "      <td>-1.050970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model     acc  sparsity      drop\n",
       "14  ww42/1008.713e_40.pth  80.654  0.255133 -0.510682\n",
       "11  ww42/1008.713e_41.pth  80.718  0.255133 -0.431736\n",
       "13  ww42/1008.713e_42.pth  80.640  0.255133 -0.527952\n",
       "12  ww42/1008.713e_43.pth  80.690  0.255133 -0.466275\n",
       "8   ww42/1008.713e_44.pth  80.664  0.255133 -0.498347\n",
       "6   ww42/1011.6586_40.pth  80.470  0.303244 -0.737652\n",
       "9   ww42/1011.6586_41.pth  80.332  0.303244 -0.907880\n",
       "1   ww42/1011.6586_42.pth  80.368  0.303244 -0.863473\n",
       "5   ww42/1011.6586_43.pth  80.370  0.303244 -0.861006\n",
       "4   ww42/1011.6586_44.pth  80.422  0.303244 -0.796862\n",
       "2   ww42/1012.689a_40.pth  80.214  0.317545 -1.053437\n",
       "3   ww42/1012.689a_41.pth  80.302  0.317545 -0.944886\n",
       "7   ww42/1012.689a_42.pth  80.248  0.317545 -1.011497\n",
       "10  ww42/1012.689a_43.pth  80.320  0.317545 -0.922682\n",
       "0   ww42/1012.689a_44.pth  80.216  0.317545 -1.050970"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "items = []\n",
    "for json_path in Path('/home/yujiepan/work2/jpqd-vit/LOGS/2023-re-collect-summary-v2').glob('*/summary.json'):\n",
    "    with open(json_path, 'r') as f:\n",
    "        result = json.load(f)\n",
    "        model = result['model_path']\n",
    "        model = model[model.index('ww42'):]\n",
    "        model = model[:14] + model[-7:]\n",
    "        items.append(dict(\n",
    "            model=model, acc=result['top1'], sparsity=result['linear_sparsity'],\n",
    "            drop=100*(result['top1'] / 81.068 - 1),\n",
    "        ))\n",
    "\n",
    "df = pd.DataFrame(items)\n",
    "df = df.sort_values(['model', 'sparsity'])\n",
    "print(df)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual crop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nncf\n",
    "import torchvision\n",
    "import shutil\n",
    "from nncf import NNCFConfig\n",
    "from nncf.torch import register_default_init_args\n",
    "from nncf.torch import create_compressed_model\n",
    "import jstyleson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_info': [{'sample_size': [1, 3, 224, 224]}], 'compression': [{'algorithm': 'movement_sparsity', 'params': {'warmup_start_epoch': 1, 'warmup_end_epoch': 4, 'importance_regularization_factor': 0.01, 'enable_structured_masking': False}, 'sparse_structure_by_scopes': [{'mode': 'block', 'sparse_factors': [32, 32], 'target_scopes': '{re}.*MultiHeadAttention*'}, {'mode': 'per_dim', 'axis': 0, 'target_scopes': '{re}.*MLPBlock.mlp./NNCFLinear.0'}, {'mode': 'per_dim', 'axis': 1, 'target_scopes': '{re}.*MLPBlock.mlp./NNCFLinear.3'}], 'ignored_scopes': ['{re}.*VisionTransformer/NNCFConv2d.conv_proj.*', '{re}.*VisionTransformer/Sequential.heads.*', '{re}.*class_token_layer.*', '{re}.*pos_embedding_layer.*']}, {'algorithm': 'quantization', 'initializer': {'range': {'num_init_samples': 0}, 'batchnorm_adaptation': {'num_bn_adaptation_samples': 0}}, 'activations': {'mode': 'symmetric'}, 'weights': {'mode': 'symmetric', 'signed': True, 'per_channel': False}}], 'log_dir': '/tmp'}\n",
      "INFO:nncf:Ignored adding weight sparsifier in scope: VisionTransformer/Sequential[heads]/NNCFLinear[head]/linear_0\n",
      "INFO:nncf:Preset quantizer parameters {'mode'} explicitly overridden by config.\n",
      "INFO:nncf:Preset quantizer parameters {'mode'} explicitly overridden by config.\n",
      "INFO:nncf:Scales will be unified for quantizer group:\n",
      "VisionTransformer/NNCFConv2d[conv_proj]/conv2d_0|OUTPUT\n",
      "VisionTransformer/NNCFEmbedding[class_token_layer]/embedding_0|OUTPUT\n",
      "\n",
      "INFO:nncf:Compiling and loading torch extension: quantized_functions_cpu...\n",
      "INFO:nncf:Finished loading torch extension: quantized_functions_cpu\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vit_b_16(weights=None, num_classes=1000)\n",
    "with open(\"/nvme2/yujiepan/workspace/jpqd-vit/LOGS/ww42/1011.6586-vit-jpqnd-wt0wr0.055-prune2to7f8-epo45lr5e-5wd1e-6_2card/vit_b16_jpqnd_2to7f8_wt0wr0.055_2card.ft.json\", 'r') as f:\n",
    "    nncf_config = jstyleson.load(f)\n",
    "\n",
    "nncf_config[\"log_dir\"] = '/tmp'\n",
    "override_qcfg_init = dict(\n",
    "    range=dict(num_init_samples=0), batchnorm_adaptation=dict(num_bn_adaptation_samples=0)\n",
    ")\n",
    "if isinstance(nncf_config[\"compression\"], list):\n",
    "    for algo in nncf_config[\"compression\"]:\n",
    "        if algo[\"algorithm\"] == \"quantization\":\n",
    "            algo[\"initializer\"].update(override_qcfg_init)\n",
    "        if algo[\"algorithm\"] == 'movement_sparsity':\n",
    "            algo['params'] =  {\n",
    "                \"warmup_start_epoch\":  1,\n",
    "                \"warmup_end_epoch\":    4,\n",
    "                \"importance_regularization_factor\":  0.01,\n",
    "                \"enable_structured_masking\":  False\n",
    "            }\n",
    "            algo['sparse_structure_by_scopes'] = [\n",
    "                {\"mode\":  \"block\",   \"sparse_factors\": [32, 32], \"target_scopes\": \"{re}.*MultiHeadAttention*\"},\n",
    "                {\"mode\":  \"per_dim\", \"axis\":  0,                 \"target_scopes\": \"{re}.*MLPBlock.mlp./NNCFLinear.0\"},\n",
    "                {\"mode\":  \"per_dim\", \"axis\":  1,                 \"target_scopes\": \"{re}.*MLPBlock.mlp./NNCFLinear.3\"},\n",
    "            ]\n",
    "elif nncf_config[\"compression\"][\"algorithm\"] == \"quantization\":\n",
    "    nncf_config[\"compression\"][\"initializer\"].update(override_qcfg_init)\n",
    "\n",
    "\n",
    "class RandDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 4\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return torch.randn((3, 224, 224)), 0\n",
    "\n",
    "fake_data_loader = torch.utils.data.DataLoader(RandDataset(), batch_size=4)\n",
    "\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config)\n",
    "nncf_config = register_default_init_args(\n",
    "nncf_config=nncf_config, train_loader=fake_data_loader\n",
    ")  # TODO: distributed_callbacks and execution_parameters\n",
    "print(nncf_config)\n",
    "\n",
    "compression_ctrl, model = create_compressed_model(model, nncf_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from state_dict_patch import resolve_structured_mask\n",
    "\n",
    "new_sd, preserved_by_layer = resolve_structured_mask(torch.load('/nvme2/yujiepan/workspace/jpqd-vit/LOGS/ww42/1011.6586-vit-jpqnd-wt0wr0.055-prune2to7f8-epo45lr5e-5wd1e-6_2card/model_40.pth')['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnnew_sd = {}\n",
    "for k, v in new_sd.items():\n",
    "    if k.startswith('nncf_module.'):\n",
    "        k = k.replace('nncf_module.', '')\n",
    "    if k.startswith('external_quantizers'):\n",
    "        k = f'_nncf.{k}'\n",
    "    if k.endswith('._weight_importance'):\n",
    "        k = k.replace('._weight_importance', '.weight_importance')\n",
    "    if k.endswith('._bias_importance'):\n",
    "        k = k.replace('._bias_importance', '.bias_importance')\n",
    "    if '/LayerNorm[' in k:\n",
    "        k = k.replace('/LayerNorm[', '/NNCFLayerNorm[')\n",
    "\n",
    "    nnnew_sd[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(nnnew_sd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
